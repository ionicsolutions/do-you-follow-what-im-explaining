# Do You Follow What I’m Explaining? A Practitioner’s Guide to Opening the AI Black Box for Humans

Kilian Kluge @ PyData Global 2022, [December 1st, 12:00 UTC, Track II](https://global2022.pydata.org/cfp/talk/T3N9MP/)

## Abstract

Methods and techniques from the realm of artificial intelligence (AI), such as machine learning,
find their way into ever more software and devices.
As more people interact with these highly complex and opaque systems in their private and professional lives,
there is a rising need to communicate AI-based decisions, predictions, and recommendations to their users.

So-called “interpretability” or “explainability” methods claim to allow insights into the proverbial “black boxes.”
Many data scientists use tools like SHAP, LIME, or partial dependence plots in their day-to-day work to analyze and debug models.

However, as numerous studies have shown, even experienced data scientists are prone to interpret the “explanations” generated 
by these tools in ways that support their pre-existing beliefs.
This problem becomes even more severe when “explanations” are presented to end-users in hopes of allowing them to
assess and scrutinize an AI system’s output.

In this talk, we’ll explore the problem space using the example of counterfactual explanations for price estimates.
Participants will learn how to employ user studies and principles from human-centric design 
to implement “explanations” that fulfill their purpose.

No prior data science knowledge is required to follow the talk, but a basic familiarity with the concept of minimizing
an objective function will be helpful.

## Further reading on topics covered in the talk

### Explanation Fundamentals

- Pacer & Lombrozo (2017): _Ockham’s Razor Cuts to the Root: Simplicity in Causal Explanation_ [doi:10.1037/xge0000318](https://doi.org/10.1037/xge0000318)
- Miller (2019): _Explanation in artificial intelligence: Insights from the social sciences_ [doi:10.1016/j.artint.2018.07.007](https://doi.org/10.1016/j.artint.2018.07.007)

### Evaluating Explanations

- Doshi-Velez & Kim (2017): _Towards A Rigorous Science of Interpretable Machine Learning_ [arXiv:1702.08608](https://arxiv.org/abs/1702.08608)
- Förster et al. (2020): _Fostering Human Agency: A Process for the Design of UserCentric XAI Systems_ [ICIS 2020 Proceedings](https://web.archive.org/web/20220802073726id_/https://aisel.aisnet.org/cgi/viewcontent.cgi?article=1064&context=icis2020)

### Generating Meaningful Explanations for Real Estate Price Predictions

- Verma et al. (2022): _Counterfactual Explanations and Algorithmic Recourses for Machine Learning: A Review_ [arXiv:2010.10596](https://arxiv.org/abs/2010.10596)
- Förster et al. (2021): _Capturing Users’ Reality: A Novel Approach to Generate Coherent Counterfactual Explanations_ [Proceedings of the 54th Hawaii International Conference on System Sciences](https://scholarspace.manoa.hawaii.edu/server/api/core/bitstreams/947e7f6b-c7b0-4dba-afcc-95c4edef0a27/content)
- Förster et al. (2022): _User-centric explainable AI: design and evaluation of an approach to generate coherent counterfactual explanations for structured data_ [doi:10.1080/12460125.2022.2119707](https://doi.org/10.1080/12460125.2022.2119707)

## Explainable AI Slack Community

- [Invite Link](https://join.slack.com/t/explainableaiworld/shared_invite/zt-1ksm9yk8k-px88Tu9_fFuU7Zl2iDFvuQ)
- If the link is no longer valid, send a message to [@XAI_Research on Twitter](https://www.twitter.com/XAI_Research) or _explainableaiworld AT gmail DOT com_ to receive an invite
## Affiliations

- [Inlinity](https://www.inlinity.ai) ([LinkedIn](https://www.linkedin.com/company/inlinity/))
- [XAI Studio](https://www.xai-studio.de) ([LinkedIn](https://www.linkedin.com/company/xai-studio/))
